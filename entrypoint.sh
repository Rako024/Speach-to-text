#!/usr/bin/env bash
set -e

echo "ğŸš€ Container baÅŸlayÄ±r..."

# GPU statusunu yoxlayÄ±rÄ±q
echo "ğŸ”§ GPU Status:"
nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader,nounits || echo "âŒ nvidia-smi É™lÃ§atmazdÄ±r"

# CUDA kitabxanalarÄ±nÄ± yoxlayÄ±rÄ±q
echo "ğŸ” CUDA Libraries:"
echo "LD_LIBRARY_PATH: $LD_LIBRARY_PATH"
ls -la /usr/lib/x86_64-linux-gnu/ | grep cudnn | head -5 || echo "âŒ cuDNN tapÄ±lmadÄ±"

# Python CUDA testini edirik
echo "ğŸ Python CUDA Test:"
python -c "
import torch
print(f'âœ… PyTorch CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'âœ… CUDA device count: {torch.cuda.device_count()}')
    print(f'âœ… Current device: {torch.cuda.current_device()}')
    print(f'âœ… Device name: {torch.cuda.get_device_name(0)}')
else:
    print('âŒ CUDA not available')
"

# faster-whisper testini edirik
echo "ğŸ™ï¸ Faster-Whisper Test:"
python -c "
try:
    from faster_whisper import WhisperModel
    print('âœ… faster-whisper import uÄŸurludur')
    # KiÃ§ik model ilÉ™ test (yÃ¼klÉ™nmÉ™sini tÉ™lÉ™b etmir)
    print('âœ… faster-whisper iÅŸlÉ™yir')
except Exception as e:
    print(f'âŒ faster-whisper xÉ™tasÄ±: {e}')
"

# Æsas script-i baÅŸladÄ±rÄ±q
echo "ğŸ¯ Æsas script baÅŸlayÄ±r: $1"

if [ "$1" = 'main' ]; then
  echo "ğŸ‘‰ Starting dispatcher (main.py)..."
  exec python3 main.py
elif [ "$1" = 'api' ]; then
  echo "ğŸ‘‰ Starting FastAPI server..."
  exec uvicorn api:app --host 0.0.0.0 --port 8000 --workers 1
elif [ "$1" = 'test' ]; then
  echo "ğŸ‘‰ Running GPU tests..."
  exec /test-gpu.sh
else
  echo "ğŸ‘‰ Running custom command: $@"
  exec "$@"
fi